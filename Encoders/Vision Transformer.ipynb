{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e63480d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:29.610108Z",
     "iopub.status.busy": "2024-10-19T22:11:29.609680Z",
     "iopub.status.idle": "2024-10-19T22:11:33.039590Z",
     "shell.execute_reply": "2024-10-19T22:11:33.038509Z"
    },
    "id": "57qkjS_qe01Q",
    "papermill": {
     "duration": 3.439699,
     "end_time": "2024-10-19T22:11:33.042378",
     "exception": false,
     "start_time": "2024-10-19T22:11:29.602679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Config:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.img_dim = 224\n",
    "\n",
    "        self.patch_dim = 16\n",
    "\n",
    "        self.channels = 3\n",
    "\n",
    "        #This would be the length of the 1D vector after flattening each patch's embedding. 16,16,3 would be flattened to 16*16*3 i.e 768\n",
    "\n",
    "        self.embed_dim = self.patch_dim * self.patch_dim * self.channels\n",
    "\n",
    "        self.attention_heads = 12\n",
    "\n",
    "        self.attention_head_size = self.embed_dim // self.attention_heads\n",
    "\n",
    "        self.transformer_layers = 3\n",
    "\n",
    "        self.classes = 10\n",
    "\n",
    "\n",
    "\n",
    "class Patcher(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        #Whenever a class involve's usage of nn Module's submodules like Conv2d, Parameter, the class needs to inherit from nn.Module's superclass hence this super() call\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #For an image of dimension (224,224) this would be equal to 224\n",
    "\n",
    "        self.img_dim = config.img_dim\n",
    "\n",
    "        #This is the size of the patch. For a patch of size 16 this would be equal to 16\n",
    "\n",
    "        self.patch_dim = config.patch_dim\n",
    "\n",
    "        #For a 3 channeled image this would be 3/ For one it would be equal to 1\n",
    "\n",
    "        self.channels = config.channels\n",
    "\n",
    "        #Number of patches for an image. If the image dimension is 224,224 and the patch dimensions are 16,16, The number of patches would be (224*224)/(16*16) == 196\n",
    "\n",
    "        self.num_patches = (config.img_dim * config.img_dim) // (config.patch_dim * config.patch_dim)\n",
    "\n",
    "        #This results in a 2D Convolution of the image where the kernel size would be the patch size and the size of the output would be (Batch_Size, embed_dim, patch_dim, patch_dim)\n",
    "\n",
    "        #A note here, stride should be set to patch_dim, since all the patches need to traversed by the transformer and the default value of 1 would be mean traversing a single pixel at a time which is not the goal\n",
    "\n",
    "        self.out = nn.Conv2d(self.channels, config.embed_dim, self.patch_dim,stride = self.patch_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.projection = self.out(x)\n",
    "\n",
    "        # print(self.projection.shape)\n",
    "\n",
    "        return self.projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cdd796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.054641Z",
     "iopub.status.busy": "2024-10-19T22:11:33.053752Z",
     "iopub.status.idle": "2024-10-19T22:11:33.065902Z",
     "shell.execute_reply": "2024-10-19T22:11:33.064861Z"
    },
    "id": "8VJBHPE0Wi8y",
    "papermill": {
     "duration": 0.020221,
     "end_time": "2024-10-19T22:11:33.068011",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.047790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "1. [CLS] TOKEN: Its a classification token which is a global representation of an image\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__() #nn.Module super() call\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #The [CLS] token which is used for classification is prepended to the positional embeddings\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.embed_dim))\n",
    "\n",
    "        #This is the positional embeddings which is a trainable paramater. Its generated for each patch of the image. Its size here is: (1,197,768)\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, ((config.img_dim // config.patch_dim) ** 2)+1, config.embed_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.patcher = Patcher()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #These are the patch embeddings generated from the Conv2D projection of the '''Patcher'''\n",
    "\n",
    "        x = self.patcher.forward(x)\n",
    "\n",
    "        #It's size currently is (batch_size, 768, 14, 14) and it needs to be flattened to (batch_size, 196,768)\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        #Batch size\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        #The [CLS] token in the __init__ is just for one image. But we need to generate tokens for all images in the batch. Hence, expand() called which simly repeats the parameter '''batch_size''' times keeping the size of the 2nd (1) and 3rd (768) dimension constant (-1)\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        #The new positional embeddings will have the [CLS] token at the start\n",
    "\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        #The new patch embeddings will be the sum of itself and the position embeddings\n",
    "\n",
    "        x = x + self.position_embeddings\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ed7815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.078718Z",
     "iopub.status.busy": "2024-10-19T22:11:33.078375Z",
     "iopub.status.idle": "2024-10-19T22:11:33.084592Z",
     "shell.execute_reply": "2024-10-19T22:11:33.083657Z"
    },
    "id": "wXX-utMPHr4S",
    "papermill": {
     "duration": 0.013867,
     "end_time": "2024-10-19T22:11:33.086659",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.072792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Softmax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        max_ = torch.max(x, dim=-1, keepdim=True).values\n",
    "\n",
    "        x = x - max_\n",
    "\n",
    "        x = torch.exp(x)\n",
    "\n",
    "        sum_ = torch.sum(x, dim = -1, keepdim=True)\n",
    "\n",
    "        return x/ sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c1e46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.097370Z",
     "iopub.status.busy": "2024-10-19T22:11:33.097069Z",
     "iopub.status.idle": "2024-10-19T22:11:33.106398Z",
     "shell.execute_reply": "2024-10-19T22:11:33.105519Z"
    },
    "id": "WpxoUlUi9b3n",
    "papermill": {
     "duration": 0.017052,
     "end_time": "2024-10-19T22:11:33.108325",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.091273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Self Attention: Because Vision Transformers are Encoder models, self attention is used wherein Q,K,V belong to the same input sequence'''\n",
    "\n",
    "'''Attention Score is calculated using the formula : softmax((Q dot K.T)/sqrt(attention_head_size)) dot V'''\n",
    "\n",
    "'''The outputs of all the heads are concatenated'''\n",
    "\n",
    "'''All Q,K,V would be of the size : (batch_size, patch_area +1, attention_head_size) or (batch_size, 197, 64)'''\n",
    "\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #This is the size of the output of each head. If the embedding size is 768 and there are 12 attention heads, the head size would be 768/12 = 64\n",
    "\n",
    "        self.attention_head_size = config.attention_head_size\n",
    "\n",
    "        #This is the Key Layer which generates the K vector for an input sequence\n",
    "\n",
    "        self.key = nn.Linear(config.embed_dim, self.attention_head_size)\n",
    "\n",
    "        #This is the Query Layer which generates the Q vector for an input sequence\n",
    "\n",
    "        self.query = nn.Linear(config.embed_dim, self.attention_head_size)\n",
    "\n",
    "        #This is the Value Layer which generates the V vector for an input sequence\n",
    "\n",
    "        self.value =nn.Linear(config.embed_dim, self.attention_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.softmax_fn = Softmax()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.query(x)\n",
    "\n",
    "        k = self.key(x)\n",
    "\n",
    "        v = self.value(x)\n",
    "\n",
    "        #This is the dot priduct of the Query Vector and the Key Vector. -2,-1 is done to reverse the 2nd dimension and the 1st dimension for a valid dot porduct\n",
    "\n",
    "        q_k_dot = torch.matmul(q, k.transpose(-2,-1))\n",
    "\n",
    "        #The caled score is dividing the attention weight by the square root of the head size (dimensionality)\n",
    "\n",
    "        scaled_score = q_k_dot / (self.attention_head_size ** 0.5)\n",
    "\n",
    "        #Softmax function\n",
    "\n",
    "        attention_score = self.softmax_fn(scaled_score)\n",
    "\n",
    "        #Dot product of the attention score with the Value vector\n",
    "\n",
    "        final_attention = torch.matmul(attention_score, v)\n",
    "\n",
    "        #The output of each head would have the size (_, 197, 64)\n",
    "\n",
    "        return final_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "587c7f49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.118433Z",
     "iopub.status.busy": "2024-10-19T22:11:33.118146Z",
     "iopub.status.idle": "2024-10-19T22:11:33.126259Z",
     "shell.execute_reply": "2024-10-19T22:11:33.125328Z"
    },
    "id": "JcTt6CkaGgDq",
    "papermill": {
     "duration": 0.015528,
     "end_time": "2024-10-19T22:11:33.128231",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.112703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionHeads(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #Making a list of all the attention heads\n",
    "\n",
    "        self.attention_heads = nn.ModuleList()\n",
    "\n",
    "        for _ in range(config.attention_heads):\n",
    "\n",
    "            head_ = SelfAttention()\n",
    "\n",
    "            #Each head's outpupt is calculated\n",
    "\n",
    "            self.attention_heads.append(head_)\n",
    "\n",
    "        #This is a trainable layer\n",
    "\n",
    "        self.output_layer = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        head_out = [k(x) for k in self.attention_heads]\n",
    "\n",
    "        #The output for each head are concatenated. So 12 outputs each of size (_,197,64) -> (_,197,768)\n",
    "\n",
    "        combined_head_out = torch.cat(head_out, dim = -1)\n",
    "\n",
    "        out = self.output_layer(combined_head_out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        # print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38eb0a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.139333Z",
     "iopub.status.busy": "2024-10-19T22:11:33.139035Z",
     "iopub.status.idle": "2024-10-19T22:11:33.144468Z",
     "shell.execute_reply": "2024-10-19T22:11:33.143526Z"
    },
    "id": "8kWq2woi0rWW",
    "papermill": {
     "duration": 0.013455,
     "end_time": "2024-10-19T22:11:33.146549",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.133094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return 0.5*x*(1 + torch.tanh(((2/3.14) ** 0.5)*(x + 0.044715 * (x**3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd984b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.158266Z",
     "iopub.status.busy": "2024-10-19T22:11:33.157980Z",
     "iopub.status.idle": "2024-10-19T22:11:33.165390Z",
     "shell.execute_reply": "2024-10-19T22:11:33.164395Z"
    },
    "id": "U-pwWk3IyXJ7",
    "papermill": {
     "duration": 0.015177,
     "end_time": "2024-10-19T22:11:33.167420",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.152243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mutli_Layer_Perceptron(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #First Connection Layer of the MLP\n",
    "\n",
    "        self.layer1 = nn.Linear(config.embed_dim, config.embed_dim//2)\n",
    "\n",
    "        #Second Connection Layer of the MLP\n",
    "\n",
    "        self.layer2 = nn.Linear(config.embed_dim//2, config.embed_dim)\n",
    "\n",
    "        #GELU Activation\n",
    "\n",
    "        self.gelu_layer = GELU()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        layer1_output = self.layer1(x)\n",
    "\n",
    "        gelu_output = self.gelu_layer(layer1_output)\n",
    "\n",
    "        layer2_output = self.layer2(gelu_output)\n",
    "\n",
    "        out = self.dropout(layer2_output)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebca4973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.178204Z",
     "iopub.status.busy": "2024-10-19T22:11:33.177918Z",
     "iopub.status.idle": "2024-10-19T22:11:33.185177Z",
     "shell.execute_reply": "2024-10-19T22:11:33.184263Z"
    },
    "id": "BUWXHJnb4Pi8",
    "papermill": {
     "duration": 0.015082,
     "end_time": "2024-10-19T22:11:33.187262",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.172180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        self.multi_attention = AttentionHeads()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        self.mlp = Mutli_Layer_Perceptron()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "      #Layer Normalization\n",
    "\n",
    "      out1 = self.layer_norm(x)\n",
    "\n",
    "      attention_out = self.multi_attention(out1)\n",
    "\n",
    "      #Skip Connection\n",
    "\n",
    "      x = x + attention_out\n",
    "\n",
    "      #Layer Normalization\n",
    "\n",
    "      out2 = self.layer_norm(x)\n",
    "\n",
    "      out3 = self.mlp(out2)\n",
    "\n",
    "      #Skip Connection\n",
    "\n",
    "      x = x + out3\n",
    "\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b411b688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.198612Z",
     "iopub.status.busy": "2024-10-19T22:11:33.197995Z",
     "iopub.status.idle": "2024-10-19T22:11:33.204628Z",
     "shell.execute_reply": "2024-10-19T22:11:33.203656Z"
    },
    "id": "GEXKLdvD8Smt",
    "papermill": {
     "duration": 0.014615,
     "end_time": "2024-10-19T22:11:33.206774",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.192159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #Transformer Blocks\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "\n",
    "        for i in range(config.transformer_layers):\n",
    "\n",
    "            block = EncoderBlock()\n",
    "\n",
    "            self.transformer_blocks.append(block)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "429a7721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.217558Z",
     "iopub.status.busy": "2024-10-19T22:11:33.217242Z",
     "iopub.status.idle": "2024-10-19T22:11:33.225400Z",
     "shell.execute_reply": "2024-10-19T22:11:33.224390Z"
    },
    "id": "18WUkwwOFcfC",
    "outputId": "b5692ef8-99ce-4441-f151-ecf5fa6d9ddc",
    "papermill": {
     "duration": 0.015936,
     "end_time": "2024-10-19T22:11:33.227551",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.211615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAll these modules need initialization of their weights and biases\\n\\nConv2d: Normal distribution of weights and 0 bias\\n\\nLinear: Normal distribution of weights and 0 bias\\n\\nLayerNorm: Weights set to 1, Bias set to 0\\n\\nParameter: There's position_embeddings and cls_token. They are set to normal distribution which as been truncated (outliers removed)\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "These are all the layers and their respective nn modules used uptil now:\n",
    "\n",
    "1. Patcher: Conv2d\n",
    "\n",
    "2. Embeddings: Parameter, Dropout\n",
    "\n",
    "3. SelfAttention: Linear, Dropout\n",
    "\n",
    "4. AttentionHeads: Linear, Dropout\n",
    "\n",
    "5. Mutli_Layer_Perceptron: Linear, Dropout\n",
    "\n",
    "6. EncoderBlock: LayerNorm\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "All these modules need initialization of their weights and biases\n",
    "\n",
    "Conv2d: Normal distribution of weights and 0 bias\n",
    "\n",
    "Linear: Normal distribution of weights and 0 bias\n",
    "\n",
    "LayerNorm: Weights set to 1, Bias set to 0\n",
    "\n",
    "Parameter: There's position_embeddings and cls_token. They are set to normal distribution which as been truncated (outliers removed)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34021e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.238695Z",
     "iopub.status.busy": "2024-10-19T22:11:33.238357Z",
     "iopub.status.idle": "2024-10-19T22:11:33.251252Z",
     "shell.execute_reply": "2024-10-19T22:11:33.250134Z"
    },
    "id": "1Ubo2RpBccHX",
    "papermill": {
     "duration": 0.021024,
     "end_time": "2024-10-19T22:11:33.253543",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.232519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = Config()\n",
    "\n",
    "        #This is the embedding layer\n",
    "\n",
    "        self.emb = Embeddings()\n",
    "\n",
    "        #This is the Encoder.\n",
    "\n",
    "        self.transformer = Transformer()\n",
    "\n",
    "        #This is the ifnal layer which works on the CLS token\n",
    "\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.classes)\n",
    "\n",
    "        self.apply(self.initialize_weights_biases)\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_weights_biases(self, layer):\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(layer, Embeddings):\n",
    "\n",
    "            layer.position_embeddings.data = nn.Parameter(nn.init.trunc_normal_(\n",
    "\n",
    "                  layer.position_embeddings.data.to(torch.float32),\n",
    "\n",
    "                  mean=0.0,\n",
    "\n",
    "                  std=0.01,\n",
    "\n",
    "              ).to(layer.position_embeddings.dtype))\n",
    "\n",
    "\n",
    "\n",
    "            layer.cls_token = nn.Parameter(nn.init.trunc_normal_(\n",
    "\n",
    "                  layer.cls_token.data.to(torch.float32),\n",
    "\n",
    "                  mean=0.0,\n",
    "\n",
    "                  std=0.01,\n",
    "\n",
    "              ).to(layer.cls_token.dtype))\n",
    "\n",
    "\n",
    "\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "\n",
    "            nn.init.normal_(layer.weight, std = 0.02)\n",
    "\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "\n",
    "        elif isinstance(layer, nn.LayerNorm):\n",
    "\n",
    "            layer.weight.data.fill_(1)\n",
    "\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "\n",
    "          nn.init.normal_(layer.weight, std = 0.02)\n",
    "\n",
    "          nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.emb(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls_token = x[:, 0, :]\n",
    "\n",
    "        out = self.final_layer(cls_token)\n",
    "\n",
    "\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf494f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T22:11:33.266167Z",
     "iopub.status.busy": "2024-10-19T22:11:33.265560Z",
     "iopub.status.idle": "2024-10-19T23:48:21.631266Z",
     "shell.execute_reply": "2024-10-19T23:48:21.630170Z"
    },
    "id": "cEJibGcKmav4",
    "outputId": "e2e8c7c1-8dcf-426c-cc47-a5435bd25216",
    "papermill": {
     "duration": 5814.151044,
     "end_time": "2024-10-19T23:48:27.410198",
     "exception": false,
     "start_time": "2024-10-19T22:11:33.259154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 78551170.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1563/1563 [04:47<00:00,  5.45it/s, loss=2.17, accuracy=22.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1745184484537945, Accuracy: 22.854%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1563/1563 [04:48<00:00,  5.42it/s, loss=2.14, accuracy=24.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.1441758984720103, Accuracy: 24.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1563/1563 [04:49<00:00,  5.40it/s, loss=2.22, accuracy=24.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2.217001100114272, Accuracy: 24.376%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1563/1563 [04:49<00:00,  5.40it/s, loss=2.24, accuracy=24.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.240109932704835, Accuracy: 24.508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1563/1563 [04:48<00:00,  5.42it/s, loss=2.16, accuracy=25.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2.161747473138918, Accuracy: 25.394%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1563/1563 [04:48<00:00,  5.42it/s, loss=2.13, accuracy=25.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 2.12752443174483, Accuracy: 25.868%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1563/1563 [04:48<00:00,  5.42it/s, loss=2.21, accuracy=26.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 2.206456618742232, Accuracy: 26.502%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1563/1563 [04:50<00:00,  5.38it/s, loss=2.2, accuracy=27.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 2.1967945001251943, Accuracy: 27.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1563/1563 [04:50<00:00,  5.38it/s, loss=2.07, accuracy=28.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 2.0673231498141056, Accuracy: 28.066%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1563/1563 [04:51<00:00,  5.37it/s, loss=2, accuracy=29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9982323814338396, Accuracy: 29.024%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1563/1563 [04:50<00:00,  5.38it/s, loss=2.03, accuracy=29.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 2.0326622954104394, Accuracy: 29.552%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 1563/1563 [04:49<00:00,  5.41it/s, loss=1.93, accuracy=30.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 1.9312089159179024, Accuracy: 30.878%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 1563/1563 [04:50<00:00,  5.39it/s, loss=1.93, accuracy=31.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 1.9277552879550712, Accuracy: 31.528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 1563/1563 [04:47<00:00,  5.43it/s, loss=1.94, accuracy=31.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 1.9391271962771717, Accuracy: 31.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 1563/1563 [04:46<00:00,  5.46it/s, loss=1.87, accuracy=33.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 1.8725772306694866, Accuracy: 33.418%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 1563/1563 [04:46<00:00,  5.45it/s, loss=1.84, accuracy=34.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 1.8420622075740452, Accuracy: 34.782%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 1563/1563 [04:47<00:00,  5.44it/s, loss=1.81, accuracy=35.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 1.813010285622175, Accuracy: 35.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 1563/1563 [04:48<00:00,  5.42it/s, loss=1.77, accuracy=36.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 1.7669029583628941, Accuracy: 36.936%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 1563/1563 [04:47<00:00,  5.44it/s, loss=1.75, accuracy=37.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 1.7513279746300277, Accuracy: 37.724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 1563/1563 [04:47<00:00,  5.43it/s, loss=1.68, accuracy=40.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 1.6795655414605095, Accuracy: 40.282%\n",
      "Test Accuracy: 45.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "model = VIT()\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    total = 0\n",
    "\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "        progress_bar.set_postfix({'loss': train_loss / (len(train_loader)), 'accuracy': 100. * correct / total})\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / len(train_loader)}, Accuracy: {100. * correct / total}%')\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Test Accuracy: {100. * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44678f55",
   "metadata": {
    "id": "UleRXoV1cjYv",
    "papermill": {
     "duration": 5.786038,
     "end_time": "2024-10-19T23:48:39.015225",
     "exception": false,
     "start_time": "2024-10-19T23:48:33.229187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca5b9e",
   "metadata": {
    "id": "jXf5GwUafD5O",
    "papermill": {
     "duration": 5.738838,
     "end_time": "2024-10-19T23:48:50.475154",
     "exception": false,
     "start_time": "2024-10-19T23:48:44.736316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5852.243393,
   "end_time": "2024-10-19T23:48:58.836687",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-19T22:11:26.593294",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
